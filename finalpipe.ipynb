{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrewesizzy/woke/blob/main/finalpipe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SCRIPT 1: MERGE PDFS FROM LEXIS NEXIS INTO ONE\n",
        "\n",
        "!pip install PyPDF2\n",
        "from PyPDF2 import PdfMerger\n",
        "\n",
        "# Example usage\n",
        "input_files = [\"/home/DissInput11.PDF\",\n",
        "               \"/home/DissInput12.PDF\"#\n",
        "               #\"/home/DissInput8.PDF\",\n",
        "               #\"/home/DissInput9.PDF\",\n",
        "               #\"/home/DissInput10.PDF\"\n",
        "               ]\n",
        "output_pdf = \"/home/Merged_2019_11-12.PDF\"\n",
        "\n",
        "def merge_pdfs(input_files, output_pdf):\n",
        "    merger = PdfMerger()\n",
        "\n",
        "    for pdf_file in input_files:\n",
        "        try:\n",
        "            with open(pdf_file, 'rb') as pdf:\n",
        "                merger.append(pdf)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"File not found: {pdf_file}\")\n",
        "\n",
        "    merger.write(output_pdf)\n",
        "    merger.close()\n",
        "\n",
        "merge_pdfs(input_files, output_pdf)"
      ],
      "metadata": {
        "id": "Q5i_NEGilTLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SCRIPT 2: USE REGULAR EXPRESSIONS TO EXTRACT BODY TEXT AND META-DATA, AND REMOVE DUPLICTES\n",
        "\n",
        "!pip install pdfplumber\n",
        "!pip install fuzzywuzzy\n",
        "import re\n",
        "import csv\n",
        "import os\n",
        "import pdfplumber\n",
        "import pandas as pd\n",
        "from dateutil import parser\n",
        "from fuzzywuzzy import fuzz\n",
        "from prettytable import PrettyTable\n",
        "from collections import Counter\n",
        "\n",
        "core_ID = \"2021.9\"\n",
        "pdf_path = \"/home/DissInput9.PDF\"\n",
        "csv_path = f\"/home/output_{core_ID}.csv\"\n",
        "\n",
        "# Dictionary mapping variations of newspaper names to a standard name\n",
        "newspaper_mapping = {\n",
        "    'The Guardian (London)': 'Guardian',\n",
        "    'The Guardian(London)': 'Guardian',\n",
        "    'DAILY MAIL (London)': 'Mail',\n",
        "    'MAIL ON SUNDAY (London)': 'Mail',\n",
        "    'Daily Mirror': 'Mirror',\n",
        "    'The Daily Telegraph (London)': 'Telegraph',\n",
        "    'The Times (London)': 'Times',\n",
        "    'The Sunday Times (London)': 'Times',\n",
        "    'Financial Times (London, England)': 'Financial',\n",
        "    'The Independent (United Kingdom)': 'Independent',\n",
        "    'The Independent - Daily Edition': 'Independent',\n",
        "    'The Sun (England)': 'Sun',\n",
        "    # Add more mappings as needed\n",
        "}\n",
        "\n",
        "newspaper_options = [\"The Guardian(London)\", \"The Guardian (London)\",\n",
        "                     \"DAILY MAIL (London)\", \"MAIL ON SUNDAY (London)\",\n",
        "                     \"Daily Mirror\", 'The Sunday Times (London)',\n",
        "                     \"The Daily Telegraph (London)\",\n",
        "                     \"The Times (London)\", \"Financial Times (London, England)\",\n",
        "                     \"The Independent (United Kingdom)\", \"The Independent - Daily Edition\",\n",
        "                     \"The Sun (England)\"]\n",
        "\n",
        "def map_newspaper_variation(newspaper_name):\n",
        "    # Function to map newspaper variation to a standard name\n",
        "    return newspaper_mapping.get(newspaper_name, 'Unknown')\n",
        "\n",
        "def extract_articles(pdf_path, csv_path, core_id):\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            with open(csv_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "                writer = csv.writer(csv_file)\n",
        "                writer.writerow(['ID', 'Copywrite', 'Author', 'Length', 'Newspaper', 'Headline', 'Date', 'Body', 'Article'])\n",
        "\n",
        "                num_articles = 0\n",
        "                for page_num, page in enumerate(pdf.pages):\n",
        "                    text = page.extract_text()\n",
        "\n",
        "                    # Check if the page contains the start of a new article\n",
        "                    if \"Length:\" in text:\n",
        "                        num_articles += 1\n",
        "                        article_id = f\"{core_id}.{num_articles}\"\n",
        "\n",
        "                        # Initialize variable to store text for the current article\n",
        "                        article_text = \"\"\n",
        "\n",
        "                        # Accumulate text from all pages of the same article\n",
        "                        for subsequent_page in pdf.pages[page_num:]:\n",
        "                            subsequent_text = subsequent_page.extract_text()\n",
        "                            article_text += subsequent_text\n",
        "\n",
        "                            # Check if the subsequent page contains the end of the article\n",
        "                            if \"End of Document\" in subsequent_text:\n",
        "                                break\n",
        "\n",
        "                        # Extract metadata and body from the accumulated text\n",
        "                        meta_match = re.search(r'^(.*?)copyright (?:2023|2022|2021|2020|2019|2018|\\d{4})\\b', article_text, re.DOTALL | re.IGNORECASE)\n",
        "                        metadata = meta_match.group(1).strip() if meta_match else 'n/a'\n",
        "\n",
        "                        pattern = rf'(.+?)\\s*({\"|\".join(re.escape(name) for name in newspaper_options)})'\n",
        "                        meta_match2 = re.search(pattern, article_text, re.DOTALL | re.IGNORECASE)\n",
        "                        # Extraxt headline\n",
        "                        double_headline = meta_match2.group(1).strip() if meta_match2 else 'n/a'\n",
        "                        # Check if four dots exist in the headline\n",
        "                        if '....' in double_headline:\n",
        "                            # Include everything after four dots\n",
        "                            headline = re.split(r'\\.{4}', double_headline)[-1].strip()\n",
        "                        else:\n",
        "                            # Remove duplicates from the headline\n",
        "                            headline_words = double_headline.split()\n",
        "                            headline = ' '.join(dict.fromkeys(headline_words))\n",
        "\n",
        "                        # Map variations of newspaper names to a standard name\n",
        "                        newspaper = 'Unknown'\n",
        "                        for variation, standard_name in newspaper_mapping.items():\n",
        "                            if variation.lower() in metadata.lower():\n",
        "                                newspaper = standard_name\n",
        "                                break\n",
        "\n",
        "                        # extract date\n",
        "                        date_match = re.search(r'(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\\s+\\d{1,2},?\\s+\\d{4}', metadata)\n",
        "                        date_str = date_match.group() if date_match else 'n/a'\n",
        "\n",
        "                        # Convert the extracted date to the desired format\n",
        "                        date = parser.parse(date_str).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "                        # Extract information using regular expressions\n",
        "                        copywrite_match = re.search(r'copyright (?:2023|2022|2021|2020|2019|2018|\\d{4})\\b(.*?)Section', article_text, re.DOTALL | re.IGNORECASE)\n",
        "                        copywrite = copywrite_match.group(1).strip() if copywrite_match else 'n/a'\n",
        "\n",
        "                        author_match = re.search(r'byline:(.*?)(?:Highlight|Body)', article_text, re.DOTALL | re.IGNORECASE)\n",
        "                        author = author_match.group(1).strip() if author_match else 'n/a'\n",
        "\n",
        "                        length_match = re.search(r'Length:\\s*(.*?)words', article_text, re.IGNORECASE)\n",
        "                        length = length_match.group(1).strip() if length_match else 'n/a'\n",
        "\n",
        "                        body_match = re.search(r'(?<=Body)(.*)End of Document', article_text, re.DOTALL | re.IGNORECASE)\n",
        "                        body = body_match.group(1).strip() if body_match else 'n/a'\n",
        "\n",
        "                        writer.writerow([\n",
        "                            article_id,\n",
        "                            copywrite,\n",
        "                            author,\n",
        "                            length,\n",
        "                            newspaper,\n",
        "                            headline,\n",
        "                            date,\n",
        "                            body,\n",
        "                            article_text.strip()\n",
        "                        ])\n",
        "\n",
        "                print(f\"Total number of articles extracted: {num_articles}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "def remove_duplicate_rows_optimized(csv_path):\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "\n",
        "        # Convert 'Body' column to string type if needed\n",
        "        df['Body'] = df['Body'].astype(str)\n",
        "\n",
        "        # Find duplicate rows based on 'Body' column\n",
        "        duplicate_mask = df.duplicated(subset='Body', keep=False)\n",
        "\n",
        "        # Keep only unique rows (non-duplicates)\n",
        "        unique_df = df[~duplicate_mask]\n",
        "\n",
        "        # Write unique data back to the CSV file\n",
        "        unique_df.to_csv(csv_path, index=False)\n",
        "\n",
        "        num_duplicates_removed = len(df) - len(unique_df)\n",
        "        print(f\"Duplicate rows removed: {num_duplicates_removed}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error while removing duplicates: {e}\")\n",
        "\n",
        "# Call the function to extract articles and write to CSV\n",
        "extract_articles(pdf_path, csv_path, core_ID)\n",
        "\n",
        "# Call the function to remove duplicate rows\n",
        "remove_duplicate_rows_optimized(csv_path)"
      ],
      "metadata": {
        "id": "EuZ1gpXpcgnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SCRIPT 3: EXPAND EACH ROW TO MAKE ON A SENTENCE BY SENTENCE BASIS\n",
        "\n",
        "import pandas as pd\n",
        "from nltk import sent_tokenize\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the CSV containing the whole body of text\n",
        "core_ID = '2021.9'\n",
        "df = pd.read_csv(f\"/home/output_{core_ID}.csv\")\n",
        "\n",
        "# Function to extract sentences containing 'woke' from body and headline texts\n",
        "def extract_sentences_with_woke(body_text, headline_text):\n",
        "    if pd.isna(body_text) or pd.isna(headline_text):\n",
        "        return []\n",
        "\n",
        "    if not isinstance(body_text, str) or not isinstance(headline_text, str):\n",
        "        return []\n",
        "\n",
        "    sentences_body = sent_tokenize(body_text)\n",
        "    woke_sentences_body = [sentence for sentence in sentences_body if re.search(r'\\bwoke\\b|\\Bwoke\\B', sentence, re.IGNORECASE)]\n",
        "\n",
        "    sentences_headline = sent_tokenize(headline_text)\n",
        "    woke_sentences_headline = [sentence for sentence in sentences_headline if re.search(r'\\bwoke\\b|\\Bwoke\\B', sentence, re.IGNORECASE)]\n",
        "\n",
        "    return woke_sentences_body + woke_sentences_headline\n",
        "\n",
        "# Apply the function to each row in the DataFrame\n",
        "df['woke_sentences'] = df.apply(lambda row: extract_sentences_with_woke(row['Body'], row['Headline']), axis=1)\n",
        "\n",
        "# Explode the DataFrame to separate each sentence into a new row\n",
        "df_exploded = df.explode('woke_sentences').reset_index(drop=True)\n",
        "\n",
        "# Remove non-ASCII characters from the 'woke_sentences' column\n",
        "df_exploded['woke_sentences'] = df_exploded['woke_sentences'].apply(lambda x: ''.join(char for char in str(x) if ord(char) < 128))\n",
        "\n",
        "# Print the number of rows before and after applying the function\n",
        "print(f\"Number of rows before: {len(df)}\")\n",
        "print(f\"Number of rows after: {len(df_exploded)}\")\n",
        "\n",
        "# Count and print the number of \"nan\"s in the 'woke_sentences' column\n",
        "nan_count = df_exploded['woke_sentences'].isnull().sum()\n",
        "print(f\"Number of 'nan's in the 'woke_sentences' column: {nan_count}\")\n",
        "\n",
        "# Save to new CSV including relevant columns\n",
        "relevant_columns = ['ID', 'Copywrite', 'Author', 'Length', 'Newspaper', 'Headline', 'Date', 'Body', 'woke_sentences']\n",
        "df_exploded[relevant_columns].to_csv(f'/home/{core_ID}_sentences_with_woke.csv', index=False, encoding='utf-8')\n",
        "\n",
        "print(\"Extraction complete. Output CSV file saved.\")\n"
      ],
      "metadata": {
        "id": "I3pSRyAwnCvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SCRIPT 4: APPLY RANDOM FOREST ON EACH ROW TO PREDICT RELEVANT OR IRRELEVANT, AND RETRURN TO ARTICLE BT ARTICLE BASIS\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "core_ID = '2021.9'\n",
        "\n",
        "# Set variables\n",
        "dataset_path = '/home/labelled_training_data_2023_1000.csv'\n",
        "df = pd.read_csv(dataset_path)\n",
        "unlabeled_dataset_path = f'/home/{core_ID}_sentences_with_woke.csv'\n",
        "\n",
        "# Specify the columns to use for training and testing\n",
        "features_column = 'woke_sentences'\n",
        "label_column = 'label'\n",
        "\n",
        "# Add a new column \"article_id\" based on the equality of specific columns\n",
        "df['article_id'] = df.groupby(['Headline', 'Author', 'Date', 'Newspaper']).ngroup()\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(df[features_column], df[label_column], test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a pipeline with TF-IDF vectorizer and Random Forest classifier\n",
        "model = make_pipeline(\n",
        "    TfidfVectorizer(),\n",
        "    RandomForestClassifier(n_estimators=200, random_state=42)\n",
        ")\n",
        "\n",
        "# Train the model on preprocessed data\n",
        "model.fit(train_data, train_labels)\n",
        "\n",
        "# Load the unlabeled dataset and handle missing values by filling NaN with an empty string\n",
        "unlabeled_df = pd.read_csv(unlabeled_dataset_path).fillna('')\n",
        "\n",
        "# Add a new column \"article_id\" based on the equality of specific columns\n",
        "unlabeled_df['new_article_id'] = unlabeled_df.groupby(['ID', 'Date', 'Newspaper']).ngroup()\n",
        "\n",
        "# Make predictions on the unlabeled dataset\n",
        "predictions = model.predict(unlabeled_df[features_column])\n",
        "\n",
        "# Create a new DataFrame to store the results\n",
        "results_df = pd.DataFrame(columns=[\"new_article_id\", \"ID\", \"Headline\", \"Newspaper\", \"Date\", \"Author\", \"woke_sentences\", \"Body\", \"prediction\"])\n",
        "\n",
        "# Counts for political and non-political articles\n",
        "political_count = 0\n",
        "non_political_count = 0\n",
        "\n",
        "# Iterate through the dataset, make predictions, and store results\n",
        "for article_id, group in unlabeled_df.groupby(\"new_article_id\"):\n",
        "    # Check if at least one sentence in the article is predicted as political\n",
        "    if 1 in predictions[group.index]:\n",
        "        political_count += 1\n",
        "\n",
        "        # Extract relevant information from the first row of the group\n",
        "        first_row = group.iloc[0]\n",
        "        new_row = {\n",
        "            \"new_article_id\": first_row['new_article_id'],\n",
        "            \"ID\": first_row['ID'],\n",
        "            \"Headline\": first_row['Headline'],\n",
        "            \"Newspaper\": first_row['Newspaper'],\n",
        "            \"Date\": first_row['Date'],\n",
        "            \"Author\": first_row['Author'],\n",
        "            \"woke_sentences\": first_row['woke_sentences'],\n",
        "            \"Body\": first_row['Body'],\n",
        "            \"prediction\": \"political\"\n",
        "        }\n",
        "        results_df = pd.concat([results_df, pd.DataFrame([new_row])], ignore_index=True)\n",
        "    else:\n",
        "        non_political_count += 1\n",
        "\n",
        "# Print the counts\n",
        "print(f\"Number of Political Articles: {political_count}\")\n",
        "print(f\"Number of Non-Political Articles: {non_political_count}\")\n",
        "\n",
        "# Save the results to a new CSV file for political articles\n",
        "results_df.to_csv(f\"/home/{core_ID}_labelled_political.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "ZNYU-AFnmcDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WORD2VEC VISUAL FOR MAIL AND GUARDIAN\n",
        "\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import SnowballStemmer\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import math\n",
        "\n",
        "# Load your CSV data\n",
        "# Replace 'your_file.csv' with the actual path to your CSV file\n",
        "df = pd.read_csv('INSERT CSV')\n",
        "\n",
        "# Initialize Snowball stemmer and list of stopwords\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "# Preprocess the text data (including replacing phrases and then stemming and stop word removal)\n",
        "def preprocess_text(text):\n",
        "    try:\n",
        "        preprocessed_text = [stemmer.stem(token) for token in simple_preprocess(text) if token not in STOPWORDS]\n",
        "        return preprocessed_text\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "df['body'] = df['Body'].apply(lambda x: preprocess_text(x))\n",
        "\n",
        "# Filter data for \"Mail\" and \"Guardian\" newspapers\n",
        "df_filtered = df[df['Newspaper'].isin(['Mail', 'Guardian'])]\n",
        "\n",
        "# Train the Word2Vec model for each newspaper\n",
        "newspapers = df_filtered['Newspaper'].unique()\n",
        "num_newspapers = len(newspapers)\n",
        "num_rows = math.ceil(math.sqrt(num_newspapers))\n",
        "num_cols = math.ceil(num_newspapers / num_rows)\n",
        "plt.figure(figsize=(20, 15))  # Larger figure size\n",
        "\n",
        "colors = ['blue', 'yellow']\n",
        "\n",
        "for i, newspaper in enumerate(newspapers):\n",
        "    newspaper_df = df_filtered[df_filtered['Newspaper'] == newspaper]\n",
        "\n",
        "    # Remove rows where preprocessing failed\n",
        "    newspaper_df = newspaper_df.dropna()\n",
        "\n",
        "    # Train the Word2Vec model\n",
        "    sentences = newspaper_df['body'].tolist()\n",
        "    try:\n",
        "        model = Word2Vec(sentences, vector_size=250, window=7, min_count=100, workers=8, seed=42, sg=1)\n",
        "    except TypeError as e:\n",
        "        print(f\"Error: {e}. Skipping newspaper '{newspaper}'.\")\n",
        "        continue\n",
        "\n",
        "    # Get the 25 most similar words to \"woke\"\n",
        "    top_words = [word for word, _ in model.wv.most_similar('woke', topn=99)]  # Adjusted to get 49 words\n",
        "\n",
        "    # Add \"woke\" to the list of words to plot\n",
        "    top_words.append('woke')\n",
        "\n",
        "    # Filter out words that are not present in the model's vocabulary\n",
        "    word_vectors = []\n",
        "    for word in top_words:\n",
        "        try:\n",
        "            word_vector = model.wv[word]\n",
        "            if isinstance(word_vector, np.ndarray):\n",
        "                word_vectors.append(word_vector)\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "    # Check if the list of word vectors is not empty\n",
        "    if word_vectors:\n",
        "        word_vectors = np.array(word_vectors)\n",
        "\n",
        "        # Apply t-SNE for dimensionality reduction\n",
        "        tsne = TSNE(n_components=2, random_state=42, perplexity=10)  # Adjust perplexity value here\n",
        "        word_vectors_2d = tsne.fit_transform(word_vectors)\n",
        "\n",
        "        # Plot the word vectors in 2D space\n",
        "        plt.subplot(num_rows, num_cols, i + 1)\n",
        "        plt.title(f'Word2Vec t-SNE projection for {newspaper} (2018-2023)', fontsize=20, fontweight='bold')  # Increase title font size and make it bold\n",
        "        plt.xlabel('Dimension 1', fontsize=15)  # Increase x-axis label font size\n",
        "        plt.ylabel('Dimension 2', fontsize=15)  # Increase y-axis label font size\n",
        "\n",
        "\n",
        "        # Loop through each point and annotate with the corresponding word\n",
        "        for j, word in enumerate(top_words):\n",
        "          if j >= len(word_vectors_2d):  # Check if j is within range\n",
        "            break  # Exit the loop if index is out of range\n",
        "\n",
        "          # Adjust marker size and transparency\n",
        "          marker_size = 30 if word == \"woke\" else 25  # Larger marker size for \"woke\"\n",
        "          marker_alpha = 1 if word == \"woke\" else 0.5  # Higher transparency for \"woke\"\n",
        "          font_weight = 2000 if word == \"woke\" else 300 # Bold for \"woke\"\n",
        "\n",
        "          # Plot markers with customized settings\n",
        "          plt.scatter(word_vectors_2d[j, 0], word_vectors_2d[j, 1], c=colors[i], label=word, edgecolor='black',\n",
        "                 s=marker_size, alpha=marker_alpha)\n",
        "\n",
        "           # Annotate each point with the corresponding word\n",
        "          plt.annotate(word, (word_vectors_2d[j, 0], word_vectors_2d[j, 1]), fontsize=15)  # Adjust annotation font size\n",
        "\n",
        "# Customize axis ticks\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hvOog7qeVPnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WORD CLOUD USING PREVIOUSLY COMPUTED COSINE SIMILARITY SCORES USING ABOVE WORD2VEC SCRIPT\n",
        "\n",
        "import pandas as pd\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Load your dataset from CSV\n",
        "df = pd.read_csv(\"/home/word2vec stemno.csv\")\n",
        "\n",
        "# Filter the DataFrame to get only the rows where \"similar word found\" is not null\n",
        "df_filtered = df.dropna(subset=['Word', 'Cosine Similarity'])\n",
        "\n",
        "# Convert the DataFrame columns to lists\n",
        "similar_words = df_filtered['Word'].tolist()\n",
        "cosine_similarities = df_filtered['Cosine Similarity'].tolist()\n",
        "\n",
        "# Create a dictionary of word frequencies based on cosine similarity scores\n",
        "word_freq = {word: cosine_similarity for word, cosine_similarity in zip(similar_words, cosine_similarities)}\n",
        "\n",
        "# Increase the frequency of \"woke\" to make it the largest\n",
        "word_freq['woke'] = max(cosine_similarities) + 0.1\n",
        "\n",
        "# Generate the word cloud with \"woke\" in the center\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='black', relative_scaling=0).generate_from_frequencies(word_freq)\n",
        "# Plot the word cloud\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud of Similar Words with Cosine Similarity Scores')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "6zHyw7NDx_qr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}